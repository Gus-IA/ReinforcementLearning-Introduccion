# Deep Q-Network (DQN) ‚Äì CartPole-v1

Este proyecto implementa un **agente de Reinforcement Learning** usando el algoritmo **Deep Q-Network (DQN)** para resolver el entorno **CartPole-v1** de Gymnasium.  
El agente aprende a equilibrar un poste sobre un carrito mediante *experiencias almacenadas*, *funci√≥n Q aproximada con redes neuronales*, y una *red objetivo* para estabilizar el aprendizaje.

---

## üöÄ Caracter√≠sticas del proyecto

- Implementaci√≥n completa de **DQN desde cero** con PyTorch.
- Uso de **Replay Memory** para romper correlaci√≥n entre muestras.
- Pol√≠tica **Œµ-greedy** con decaimiento exponencial.
- Actualizaci√≥n suave (**Soft Update**) de la red objetivo mediante `TAU`.
- Entrenamiento autom√°tico con soporte para:
  - **CUDA (GPU NVIDIA)**
  - **MPS (GPU Apple Silicon)**
  - **CPU**
- Gr√°ficas en tiempo real del progreso del agente.
- C√≥digo totalmente documentado paso a paso.

---

## üìÇ Estructura general del algoritmo aprendido

### 1. **Transiciones (Transition)**
Cada experiencia del agente guarda:
- estado
- acci√≥n
- siguiente estado
- recompensa

Se almacena en una estructura `namedtuple`, ideal por su velocidad y simplicidad.

---

### 2. **ReplayMemory**
Gesti√≥n de memoria FIFO con capacidad limitada.  
Permite extraer batches aleatorios para entrenar la red.

Esto:
- evita que el agente aprenda solo de experiencias recientes,
- mejora la estabilidad del entrenamiento.

---

### 3. **Red neuronal (DQN)**
Tres capas lineales con 128 neuronas intermedias:


Usa activaci√≥n ReLU y salida sin activaci√≥n para representar los valores Q(s, a).

---

### 4. **Pol√≠tica Œµ-greedy**
El agente:
- explora (acci√≥n aleatoria) con probabilidad Œµ,
- explota (mejor acci√≥n) con prob. 1‚àíŒµ,

donde Œµ decae exponencialmente desde 0.9 hasta 0.01.

---

### 5. **C√°lculo de la funci√≥n Q y Backpropagation**
Durante el entrenamiento se calcula:

\[
Q_{\text{target}} = r + \gamma \max_{a'} Q_{\text{target}}(s', a')
\]

La p√©rdida se calcula con **SmoothL1Loss (Huber Loss)**.

---

### 6. **Soft Update de la red objetivo**
La red objetivo se actualiza lentamente:

\[
\theta_{target} \leftarrow \tau \theta_{policy} + (1-\tau)\theta_{target}
\]

Esto evita oscilaciones en el entrenamiento.

---

### 7. **Entrenamiento**
Se ejecutan entre:
- **600 episodios en GPU**,  
- **50 episodios en CPU**,  

seg√∫n disponibilidad del hardware.

---

## üìä Gr√°ficas del rendimiento

El script dibuja la duraci√≥n de cada episodio y una media m√≥vil de 100 episodios.  
Se utiliza `matplotlib` en modo interactivo.

---

üß© Requisitos

Antes de ejecutar el script, instala las dependencias:

pip install -r requirements.txt

üßë‚Äçüíª Autor

Desarrollado por Gus como parte de su aprendizaje en Python e IA.
